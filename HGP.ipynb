{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HGP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPc902aiB2P3QdwTTZi/yHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiSaphire/Colaboratory/blob/master/HGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZN2eBlR4uoa",
        "colab_type": "text"
      },
      "source": [
        "# HGP Internship Assignment\n",
        "## According to given Problem Statement:\n",
        "**Given an input string from a user, I need to parse it into components to be used for further processing.\n",
        "These components will be best matches against predefined lists and / or scalars.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQf_Gz53P8aD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw3Cdm2m5tcM",
        "colab_type": "text"
      },
      "source": [
        "## My Approach to problem\n",
        "Divide and conquer all todos separately.\n",
        "**For detailed working of Solution please refer the PDF provided**\n",
        "\n",
        "**Entire Processâ€‹ -**\n",
        "1. Use regex to separate Time period and Unit of time period and split them to store them separately.\n",
        "2. Preprocess the Data.\n",
        "3. Separate Sectors by topic modelling using LDA, again use topic modelling to get sub-topics or fundamentals with corpus of fundamental_docs.\n",
        "4. Use Contextual similarity and Syntactic similarity algorithms of separated Sector names and fundamental name (for proper spell check and synonym check).\n",
        "5. Store Sector, Fundamental, Attributes of Fundamentals, Time Period, Unit of Time period as keys in a dictionary and append their values. Return dictionary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnWT6Ji8_2cI",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Using Regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LfJqZUH9dIu",
        "colab_type": "code",
        "outputId": "b46ca554-f90b-43f1-8196-d2b3a1cc12f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "string = \"Output Revenue, EBITDA margin for Steel and Metal stocks for past 10 qtrs\"\n",
        "match = re.search(\"\\d+\\s*\\w+\", string)\n",
        "print(match.group())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 qtrs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsw0vgqE_-65",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdIowGjr_NmV",
        "colab_type": "code",
        "outputId": "5f0493cc-149b-42a0-8f0d-aa457c4bbb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "      result.append(lemmatize_stemming(token))\n",
        "  return result\n",
        "\n",
        "processed_string = preprocess(string)\n",
        "print(processed_string)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['output', 'revenu', 'ebitda', 'margin', 'steel', 'metal', 'stock', 'past', 'qtr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9CJaqEpAMU8",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 - BoW Conversion and Applying LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VommnKQYAcgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sectors_docs need to be supplied to create a Bag of Words from it.\n",
        "\n",
        "import pandas as pd\n",
        "sectors_docs = pd.read_csv(\"Some_Sector_Document.tsv\",delimiter =\"\\t\", quoting =3)\n",
        "dictionary = gensim.corpora.Dictionary(sectors_docs)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in sectors_docs]\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 2, id2word = dictionary, passes = 10, workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyD0w_WtCALW",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 - Testing LDA model on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdiZffXiMjwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vector = dictionary.doc2bow(processed_string)\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-2zwapATFNS",
        "colab_type": "text"
      },
      "source": [
        "## Step 5 - Handling Contextual Similarity using WordNets synsets (Pseudo Code)\n",
        "\n",
        "This is a Pseudo code for checking contextual similarity between two words and\n",
        "returning True if words are more than 70% similar.\n",
        "\n",
        "While developing we can check this with processed_string against bow_corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCG0yMsTQuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51885b42-431b-41b0-984f-6c651cff4ec7"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import product\n",
        "\n",
        "def contextual_similarity():\n",
        "  wordx, wordy = \"revenue\",\"sales\"\n",
        "  sem1, sem2 = wn.synsets(wordx), wn.synsets(wordy)\n",
        "  maxscore = 0\n",
        "  for i,j in list(product(sem1,sem2)):\n",
        "    score = i.wup_similarity(j)\n",
        "    maxscore = score if maxscore < score else maxscore\n",
        "  return True if maxscore > 0.70 else False\n",
        "\n",
        "print(contextual_similarity())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huiwzk1iWVzC",
        "colab_type": "text"
      },
      "source": [
        "## Step 6 - Handling Syntactical Similarity using SymSpell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIiuDx7HWriW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for line in fetch_20newsgroups().data:\n",
        "  line = line.replace('\\n', ' ').replace('\\t', ' ').lower()                              #This is a Pseudo Code\n",
        "  line = re.sub('[^a-z ]', ' ', line)                                                    #Here newspaper corpus is made\n",
        "  tokens = line.split(' ')                                                               #Just to show how SymSpell works\n",
        "  tokens = [token for token in tokens if len(token) > 0]                                 #While building real model we can use suitable corpus\n",
        "  corpus.extend(tokens)\n",
        "corpus = Counter(corpus)\n",
        "corpus_dir = '../'\n",
        "corpus_file_name = 'dorian_gray.txt'\n",
        "symspell = SymSpell(verbose=10)\n",
        "symspell.build_vocab(dictionary=corpus, file_dir=corpus_dir, file_name=corpus_file_name)\n",
        "symspell.load_vocab(corpus_file_path=corpus_dir+corpus_file_name)\n",
        "results = symspell.correction(word='helol')\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}