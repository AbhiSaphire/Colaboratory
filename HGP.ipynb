{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HGP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyPRTT+ftU9fvg7GE+J/ku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiSaphire/Colaboratory/blob/master/HGP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZN2eBlR4uoa",
        "colab_type": "text"
      },
      "source": [
        "# HGP Internship Assignment\n",
        "## According to given Problem Statement:\n",
        "**Given an input string from a user, I need to parse it into components to be used for further processing.\n",
        "These components will be best matches against predefined lists and / or scalars.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQf_Gz53P8aD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw3Cdm2m5tcM",
        "colab_type": "text"
      },
      "source": [
        "## My Approach to problem\n",
        "Divide and conquer all todos separately.\n",
        "**For detailed working of Solution please refer the PDF provided**\n",
        "\n",
        "**Entire Process​ -**\n",
        "1. Use regex to separate Time period and Unit of time period and split them to store them separately.\n",
        "2. Preprocess the Data.\n",
        "3. Separate Sectors by topic modelling using LDA, again use topic modelling to get sub-topics or fundamentals with corpus of fundamental_docs.\n",
        "4. Use Contextual similarity and Syntactic similarity algorithms of separated Sector names and fundamental name (for proper spell check and synonym check).\n",
        "5. Store Sector, Fundamental, Attributes of Fundamentals, Time Period, Unit of Time period as keys in a dictionary and append their values. Return dictionary!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnWT6Ji8_2cI",
        "colab_type": "text"
      },
      "source": [
        "## Step 1 - Using Regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LfJqZUH9dIu",
        "colab_type": "code",
        "outputId": "b46ca554-f90b-43f1-8196-d2b3a1cc12f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "string = \"Output Revenue, EBITDA margin for Steel and Metal stocks for past 10 qtrs\"\n",
        "match = re.search(\"\\d+\\s*\\w+\", string)\n",
        "print(match.group())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 qtrs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsw0vgqE_-65",
        "colab_type": "text"
      },
      "source": [
        "## Step 2 - Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdIowGjr_NmV",
        "colab_type": "code",
        "outputId": "5f0493cc-149b-42a0-8f0d-aa457c4bbb68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "  result=[]\n",
        "  for token in gensim.utils.simple_preprocess(text) :\n",
        "    if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "      result.append(lemmatize_stemming(token))\n",
        "  return result\n",
        "\n",
        "processed_string = preprocess(string)\n",
        "print(processed_string)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['output', 'revenu', 'ebitda', 'margin', 'steel', 'metal', 'stock', 'past', 'qtr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9CJaqEpAMU8",
        "colab_type": "text"
      },
      "source": [
        "## Step 3 - BoW Conversion and Applying LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VommnKQYAcgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sectors_docs need to be supplied to create a Bag of Words from it.\n",
        "\n",
        "import pandas as pd\n",
        "sectors_docs = pd.read_csv(\"Some_Sector_Document.tsv\",delimiter =\"\\t\", quoting =3)\n",
        "dictionary = gensim.corpora.Dictionary(sectors_docs)\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in sectors_docs]\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 2, id2word = dictionary, passes = 10, workers = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyD0w_WtCALW",
        "colab_type": "text"
      },
      "source": [
        "## Step 4 - Testing LDA model on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdiZffXiMjwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vector = dictionary.doc2bow(processed_string)\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-2zwapATFNS",
        "colab_type": "text"
      },
      "source": [
        "## Step 5 - Handling **Contextual Similarity** using **WordNets synsets** (Pseudo Code)\n",
        "\n",
        "This is a Pseudo code for checking contextual similarity between two words and\n",
        "returning True if words are more than 70% similar.\n",
        "\n",
        "While developing we can check this with processed_string against bow_corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCG0yMsTQuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51885b42-431b-41b0-984f-6c651cff4ec7"
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from itertools import product\n",
        "\n",
        "def contextual_similarity():\n",
        "  wordx, wordy = \"revenue\",\"sales\"\n",
        "  sem1, sem2 = wn.synsets(wordx), wn.synsets(wordy)\n",
        "  maxscore = 0\n",
        "  for i,j in list(product(sem1,sem2)):\n",
        "    score = i.wup_similarity(j)\n",
        "    maxscore = score if maxscore < score else maxscore\n",
        "  return True if maxscore > 0.70 else False\n",
        "\n",
        "print(contextual_similarity())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huiwzk1iWVzC",
        "colab_type": "text"
      },
      "source": [
        "## Step 6 - Handling **Syntactical Similarity** using **SymSpell**\n",
        "\n",
        "● **“qtrs”, “quarters”** should match / **“years”, “yr”**​ . These types of\n",
        "problems can be handled by lemmatization and stemming, which was a\n",
        "part of preprocessing we did. Every word will be changed to its root form.\n",
        "\n",
        "● Handling spelling mistakes would be a big task here, the best I know is to\n",
        "use **SymSpell** library bacause it is 100 times (if not 1000 times) faster than other algorithms like **BK-tree**. The only trick here is SymSpell is a C# library but using it in python is not that difficult."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIiuDx7HWriW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8c372a1d-1c99-4a92-e89e-edabde5f570d"
      },
      "source": [
        "!pip install -U symspellpy                                                      # Installing SymSpell because its a c# library\n",
        "\n",
        "import pkg_resources\n",
        "from symspellpy.symspellpy import SymSpell, Verbosity\n",
        "\n",
        "def spell_check_using_SymSpell():\n",
        "    max_edit_distance_dictionary = 2\n",
        "    prefix_length = 7\n",
        "    sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
        "    dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "    bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
        "    if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
        "        print(\"Dictionary file not found\")\n",
        "        return\n",
        "    if not sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
        "        print(\"Bigram dictionary file not found\")\n",
        "        return\n",
        "\n",
        "    input_term = \"revene\"                                                       # misspelling of \"revenue\"\n",
        "    max_edit_distance_lookup = 2\n",
        "    suggestion_verbosity = Verbosity.TOP\n",
        "    suggestions = sym_spell.lookup(input_term, suggestion_verbosity, max_edit_distance_lookup)\n",
        "    for suggestion in suggestions:\n",
        "        print(\"{}, {}, {}\".format(suggestion.term, suggestion.distance, suggestion.count))\n",
        "\n",
        "    input_term = (\"Pleaes providean approch to solvingthe above. Soluton maynclude elements of NLP, MachinLearning, regex etc.\")\n",
        "    max_edit_distance_lookup = 2\n",
        "    suggestions = sym_spell.lookup_compound(input_term, max_edit_distance_lookup)\n",
        "    for suggestion in suggestions:\n",
        "        print(\"{}, {}, {}\".format(suggestion.term, suggestion.distance, suggestion.count))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spell_check_using_SymSpell()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: symspellpy in /usr/local/lib/python3.6/dist-packages (6.5.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.18.4)\n",
            "revenue, 1, 32088216\n",
            "please provide an approach to solving the above solution may include elements of nip machine learning regex etc, 20, 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dH8eLDKobSi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}